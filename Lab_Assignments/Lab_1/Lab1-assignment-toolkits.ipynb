{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1-Assignment\n",
    "\n",
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "This notebook describes the assignment for Lab 1 of the text mining course. \n",
    "\n",
    "**Points**: each exercise is prefixed with the number of points you can obtain for the exercise.\n",
    "\n",
    "We assume you have worked through the following notebooks:\n",
    "* **Lab1.1-introduction**\n",
    "* **Lab1.2-introduction-to-NLTK**\n",
    "* **Lab1.3-introduction-to-spaCy** \n",
    "\n",
    "In this assignment, you will process an English text (**Lab1-apple-samsung-example.txt**) with both NLTK and spaCy and discuss the similarities and differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip: how to read a file from disk\n",
    "Let's open the file **Lab1-apple-samsung-example.txt** from disk."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:07.057255Z",
     "start_time": "2025-02-14T13:27:07.055318Z"
    }
   },
   "source": [
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:07.631915Z",
     "start_time": "2025-02-14T13:27:07.628294Z"
    }
   },
   "source": [
    "cur_dir = Path().resolve() # this should provide you with the folder in which this notebook is placed\n",
    "path_to_file = Path.joinpath(cur_dir, 'Lab1-apple-samsung-example.txt')\n",
    "print(path_to_file)\n",
    "print('does path exist? ->', Path.exists(path_to_file))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/robertostoica/Desktop/University_files_(year_3)_extra/period_4/Text Mining for AI/Assignments/Lab_Assignments/Lab_1/Lab1-apple-samsung-example.txt\n",
      "does path exist? -> True\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output from the code cell above states that **does path exist? -> False**, please check that the file **Lab1-apple-samsung-example.txt** is in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:12.178071Z",
     "start_time": "2025-02-14T13:27:12.174060Z"
    }
   },
   "source": [
    "with open(path_to_file) as infile:\n",
    "    text = infile.read()\n",
    "\n",
    "print('number of characters', len(text))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters 1139\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [total points: 4] Exercise 1: NLTK\n",
    "In this exercise, we use NLTK to apply **Part-of-speech (POS) tagging**, **Named Entity Recognition (NER)**, and **Constituency parsing**. The following code snippet already performs sentence splitting and tokenization. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:21.711554Z",
     "start_time": "2025-02-14T13:27:21.127821Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:22.370156Z",
     "start_time": "2025-02-14T13:27:22.347499Z"
    }
   },
   "source": [
    "sentences_nltk = sent_tokenize(text)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:23.181139Z",
     "start_time": "2025-02-14T13:27:23.176608Z"
    }
   },
   "source": [
    "tokens_per_sentence = []\n",
    "for sentence_nltk in sentences_nltk:\n",
    "    sent_tokens = word_tokenize(sentence_nltk)\n",
    "    tokens_per_sentence.append(sent_tokens)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use lists to keep track of the output of the NLP tasks. We can hence inspect the output for each task using the index of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:25.515466Z",
     "start_time": "2025-02-14T13:27:25.512353Z"
    }
   },
   "source": [
    "sent_id = 1\n",
    "print('SENTENCE', sentences_nltk[sent_id])\n",
    "print('TOKENS', tokens_per_sentence[sent_id])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE The six phones and tablets affected are the Galaxy S III, running the new Jelly Bean system, the Galaxy Tab 8.9 Wifi tablet, the Galaxy Tab 2 10.1, Galaxy Rugby Pro and Galaxy S III mini.\n",
      "TOKENS ['The', 'six', 'phones', 'and', 'tablets', 'affected', 'are', 'the', 'Galaxy', 'S', 'III', ',', 'running', 'the', 'new', 'Jelly', 'Bean', 'system', ',', 'the', 'Galaxy', 'Tab', '8.9', 'Wifi', 'tablet', ',', 'the', 'Galaxy', 'Tab', '2', '10.1', ',', 'Galaxy', 'Rugby', 'Pro', 'and', 'Galaxy', 'S', 'III', 'mini', '.']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [point: 1] Exercise 1a: Part-of-speech (POS) tagging\n",
    "Use `nltk.pos_tag` to perform part-of-speech tagging on each sentence.\n",
    "\n",
    "Use `print` to **show** the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:30.146530Z",
     "start_time": "2025-02-14T13:27:30.143282Z"
    }
   },
   "source": [
    "pos_tags_per_sentence = []\n",
    "for tokens in tokens_per_sentence:\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:32.471780Z",
     "start_time": "2025-02-14T13:27:32.468631Z"
    }
   },
   "source": [
    "print(pos_tags_per_sentence)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [point: 1] Exercise 1b: Named Entity Recognition (NER)\n",
    "Use `nltk.chunk.ne_chunk` to perform Named Entity Recognition (NER) on each sentence.\n",
    "\n",
    "Use `print` to **show** the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:34.120187Z",
     "start_time": "2025-02-14T13:27:34.117309Z"
    }
   },
   "source": [
    "ner_tags_per_sentence = []"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:34.835998Z",
     "start_time": "2025-02-14T13:27:34.832660Z"
    }
   },
   "source": [
    "print(ner_tags_per_sentence)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 2] Exercise 1c: Constituency parsing\n",
    "Use the `nltk.RegexpParser` to perform constituency parsing on each sentence.\n",
    "\n",
    "Use `print` to **show** the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:41.718859Z",
     "start_time": "2025-02-14T13:27:41.713799Z"
    }
   },
   "source": [
    "constituent_parser = nltk.RegexpParser('''\n",
    "NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "P: {<IN>}           # Preposition\n",
    "V: {<V.*>}          # Verb\n",
    "PP: {<P> <NP>}      # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*''')"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:42.328054Z",
     "start_time": "2025-02-14T13:27:42.325119Z"
    }
   },
   "source": [
    "constituency_output_per_sentence = []"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:43.003785Z",
     "start_time": "2025-02-14T13:27:43.001513Z"
    }
   },
   "source": [
    "print(constituency_output_per_sentence)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the RegexpParser so that it also detects Named Entity Phrases (NEP), e.g., that it detects *Galaxy S III* and *Ice Cream Sandwich*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:44.591154Z",
     "start_time": "2025-02-14T13:27:44.587566Z"
    }
   },
   "source": [
    "constituent_parser_v2 = nltk.RegexpParser('''\n",
    "NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "P: {<IN>}           # Preposition\n",
    "V: {<V.*>}          # Verb\n",
    "PP: {<P> <NP>}      # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*\n",
    "NEP: {}             # ???''')"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:47.055507Z",
     "start_time": "2025-02-14T13:27:47.052898Z"
    }
   },
   "source": [
    "constituency_v2_output_per_sentence = []"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:47.659323Z",
     "start_time": "2025-02-14T13:27:47.656157Z"
    }
   },
   "source": [
    "print(constituency_v2_output_per_sentence)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [total points: 1] Exercise 2: spaCy\n",
    "Use Spacy to process the same text as you analyzed with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:28:18.532135Z",
     "start_time": "2025-02-14T13:28:18.485339Z"
    }
   },
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config file from /Users/robertostoica/Desktop/anaconda3/envs/Text_Mining_python/lib/python3.12/site-packages/en_core_web_sm/en_core_web_sm-2.1.0/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m nlp \u001B[38;5;241m=\u001B[39m spacy\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/anaconda3/envs/Text_Mining_python/lib/python3.12/site-packages/spacy/__init__.py:51\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     28\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m util\u001B[38;5;241m.\u001B[39mload_model(\n\u001B[1;32m     52\u001B[0m         name,\n\u001B[1;32m     53\u001B[0m         vocab\u001B[38;5;241m=\u001B[39mvocab,\n\u001B[1;32m     54\u001B[0m         disable\u001B[38;5;241m=\u001B[39mdisable,\n\u001B[1;32m     55\u001B[0m         enable\u001B[38;5;241m=\u001B[39menable,\n\u001B[1;32m     56\u001B[0m         exclude\u001B[38;5;241m=\u001B[39mexclude,\n\u001B[1;32m     57\u001B[0m         config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[1;32m     58\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/anaconda3/envs/Text_Mining_python/lib/python3.12/site-packages/spacy/util.py:465\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m get_lang_class(name\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblank:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))()\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_package(name):  \u001B[38;5;66;03m# installed as package\u001B[39;00m\n\u001B[0;32m--> 465\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_package(name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Path(name)\u001B[38;5;241m.\u001B[39mexists():  \u001B[38;5;66;03m# path to model data directory\u001B[39;00m\n\u001B[1;32m    467\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_path(Path(name), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/anaconda3/envs/Text_Mining_python/lib/python3.12/site-packages/spacy/util.py:501\u001B[0m, in \u001B[0;36mload_model_from_package\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    484\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load a model from an installed package.\u001B[39;00m\n\u001B[1;32m    485\u001B[0m \n\u001B[1;32m    486\u001B[0m \u001B[38;5;124;03mname (str): The package name.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    498\u001B[0m \u001B[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(name)\n\u001B[0;32m--> 501\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mload(vocab\u001B[38;5;241m=\u001B[39mvocab, disable\u001B[38;5;241m=\u001B[39mdisable, enable\u001B[38;5;241m=\u001B[39menable, exclude\u001B[38;5;241m=\u001B[39mexclude, config\u001B[38;5;241m=\u001B[39mconfig)\n",
      "File \u001B[0;32m~/Desktop/anaconda3/envs/Text_Mining_python/lib/python3.12/site-packages/en_core_web_sm/__init__.py:12\u001B[0m, in \u001B[0;36mload\u001B[0;34m(**overrides)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moverrides):\n\u001B[0;32m---> 12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_init_py(\u001B[38;5;18m__file__\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moverrides)\n",
      "File \u001B[0;32m~/Desktop/anaconda3/envs/Text_Mining_python/lib/python3.12/site-packages/spacy/util.py:682\u001B[0m, in \u001B[0;36mload_model_from_init_py\u001B[0;34m(init_file, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    680\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_path\u001B[38;5;241m.\u001B[39mexists():\n\u001B[1;32m    681\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE052\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mdata_path))\n\u001B[0;32m--> 682\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_path(\n\u001B[1;32m    683\u001B[0m     data_path,\n\u001B[1;32m    684\u001B[0m     vocab\u001B[38;5;241m=\u001B[39mvocab,\n\u001B[1;32m    685\u001B[0m     meta\u001B[38;5;241m=\u001B[39mmeta,\n\u001B[1;32m    686\u001B[0m     disable\u001B[38;5;241m=\u001B[39mdisable,\n\u001B[1;32m    687\u001B[0m     enable\u001B[38;5;241m=\u001B[39menable,\n\u001B[1;32m    688\u001B[0m     exclude\u001B[38;5;241m=\u001B[39mexclude,\n\u001B[1;32m    689\u001B[0m     config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[1;32m    690\u001B[0m )\n",
      "File \u001B[0;32m~/Desktop/anaconda3/envs/Text_Mining_python/lib/python3.12/site-packages/spacy/util.py:538\u001B[0m, in \u001B[0;36mload_model_from_path\u001B[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    536\u001B[0m config_path \u001B[38;5;241m=\u001B[39m model_path \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig.cfg\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    537\u001B[0m overrides \u001B[38;5;241m=\u001B[39m dict_to_dot(config, for_overrides\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 538\u001B[0m config \u001B[38;5;241m=\u001B[39m load_config(config_path, overrides\u001B[38;5;241m=\u001B[39moverrides)\n\u001B[1;32m    539\u001B[0m nlp \u001B[38;5;241m=\u001B[39m load_model_from_config(\n\u001B[1;32m    540\u001B[0m     config,\n\u001B[1;32m    541\u001B[0m     vocab\u001B[38;5;241m=\u001B[39mvocab,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    545\u001B[0m     meta\u001B[38;5;241m=\u001B[39mmeta,\n\u001B[1;32m    546\u001B[0m )\n\u001B[1;32m    547\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m nlp\u001B[38;5;241m.\u001B[39mfrom_disk(model_path, exclude\u001B[38;5;241m=\u001B[39mexclude, overrides\u001B[38;5;241m=\u001B[39moverrides)\n",
      "File \u001B[0;32m~/Desktop/anaconda3/envs/Text_Mining_python/lib/python3.12/site-packages/spacy/util.py:714\u001B[0m, in \u001B[0;36mload_config\u001B[0;34m(path, overrides, interpolate)\u001B[0m\n\u001B[1;32m    712\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    713\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m config_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m config_path\u001B[38;5;241m.\u001B[39mis_file():\n\u001B[0;32m--> 714\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE053\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mconfig_path, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig file\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    715\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m config\u001B[38;5;241m.\u001B[39mfrom_disk(\n\u001B[1;32m    716\u001B[0m         config_path, overrides\u001B[38;5;241m=\u001B[39moverrides, interpolate\u001B[38;5;241m=\u001B[39minterpolate\n\u001B[1;32m    717\u001B[0m     )\n",
      "\u001B[0;31mOSError\u001B[0m: [E053] Could not read config file from /Users/robertostoica/Desktop/anaconda3/envs/Text_Mining_python/lib/python3.12/site-packages/en_core_web_sm/en_core_web_sm-2.1.0/config.cfg"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T13:27:54.427375Z",
     "start_time": "2025-02-14T13:27:54.413342Z"
    }
   },
   "source": [
    "doc = nlp(text) # insert code here"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m doc \u001B[38;5;241m=\u001B[39m nlp(text)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small tip: You can use **sents = list(doc.sents)** to be able to use the index to access a sentence like **sents[2]** for the third sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [total points: 7] Exercise 3: Comparison NLTK and spaCy\n",
    "We will now compare the output of NLTK and spaCy, i.e., in what do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 3] Exercise 3a: Part of speech tagging\n",
    "Compare the output from NLTK and spaCy regarding part of speech tagging.\n",
    "\n",
    "* To compare, you probably would like to compare sentence per sentence. Describe if the sentence splitting is different for NLTK than for spaCy. If not, where do they differ?\n",
    "* After checking the sentence splitting, select a sentence for which you expect interesting results and perhaps differences. Motivate your choice.\n",
    "* Compare the output in `token.tag` from spaCy to the part of speech tagging from NLTK for each token in your selected sentence. Are there any differences? This is not a trick question; it is possible that there are no differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 2] Exercise 3b: Named Entity Recognition (NER)\n",
    "* Describe differences between the output from NLTK and spaCy for Named Entity Recognition. Which one do you think performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 2] Exercise 3c: Constituency/dependency parsing\n",
    "Choose one sentence from the text and run constituency parsing using NLTK and dependency parsing using spaCy.\n",
    "* describe briefly the difference between constituency parsing and dependency parsing\n",
    "* describe differences between the output from NLTK and spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
